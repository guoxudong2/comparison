'''import os
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from urllib.parse import urljoin, urlparse

# è®¤è¯ä¿¡æ¯
username = "nedc-tuh-eeg"
password = "L5!Np#8Z$7xJmRpK"

# ç›®æ ‡èµ·å§‹ç›®å½•ï¼ˆåªä¸‹è½½ `edf/` åŠä»¥ä¸‹å†…å®¹ï¼‰
base_url = "https://isip.piconepress.com/projects/nedc/data/tuh_eeg/tuh_eeg_events/v2.0.1/edf/"
save_dir = "./downloads/tuh_eeg_events_v2.0.1"

# è®°å½•å·²è®¿é—®çš„ç›®å½•ï¼Œé¿å…æ— é™é€’å½’
visited_dirs = set()

# åˆ›å»ºä¼šè¯ï¼Œå¸¦ä¸Šèº«ä»½è®¤è¯
session = requests.Session()
session.auth = (username, password)


def get_file_links(url):
    """ è·å–ç½‘é¡µä¸­æ‰€æœ‰æ–‡ä»¶å’Œå­ç›®å½•é“¾æ¥ """
    response = session.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    links = []
    for link in soup.find_all("a"):
        href = link.get("href")
        full_url = urljoin(url, href)
        # **è¿‡æ»¤æ‰æ— æ•ˆé“¾æ¥**
        if href and not href.startswith("?") and not href.startswith("Parent Directory") and not href.startswith("../"):
            if full_url.startswith(base_url):  # ç¡®ä¿ä¸è·³å‡º `edf/` ç›®å½•
                links.append(href)

    return links


def sanitize_filename(url):
    """ å¤„ç†éæ³•æ–‡ä»¶å """
    parsed_url = urlparse(url)
    filename = os.path.basename(parsed_url.path)
    return filename.replace("?", "_").replace(";", "_").replace("=", "_")


def download_file(file_url, local_path):
    """ ä¸‹è½½å•ä¸ªæ–‡ä»¶ """
    if os.path.exists(local_path):
        print(f"âœ… å·²å­˜åœ¨ï¼Œè·³è¿‡: {local_path}")
        return

    print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½: {file_url}")
    response = session.get(file_url, stream=True)
    response.raise_for_status()

    with open(local_path, "wb") as f:
        for chunk in tqdm(response.iter_content(chunk_size=8192), desc=os.path.basename(local_path)):
            f.write(chunk)


def recursive_download(url, local_dir):
    """ é€’å½’ä¸‹è½½æ•´ä¸ª `edf/` åŠä»¥ä¸‹çš„ `train/` `eval/` ç›®å½•å’Œ `.edf` `.lab` æ–‡ä»¶ """
    if url in visited_dirs:
        print(f"âš ï¸ å·²è®¿é—®ï¼Œè·³è¿‡: {url}")
        return  # é¿å…æ­»å¾ªç¯

    visited_dirs.add(url)  # è®°å½•å½“å‰è®¿é—®çš„ç›®å½•
    os.makedirs(local_dir, exist_ok=True)  # **ä¿æŒåŸå§‹ç›®å½•ç»“æ„**

    links = get_file_links(url)

    for link in links:
        full_url = urljoin(url, link)
        local_path = os.path.join(local_dir, link)  # **ä¿æŒåŸå§‹ç›®å½•ç»“æ„**

        if link.endswith("/") and full_url.startswith(base_url):  # ç›®å½•ï¼Œé€’å½’ä¸‹è½½
            print(f"ğŸ“‚ è¿›å…¥å­ç›®å½•: {full_url}")
            recursive_download(full_url, local_path)
        elif (link.endswith(".edf") or link.endswith(".lab") or link.endswith(".rec") or link.endswith(".htk")) and full_url.startswith(base_url):  # ä¸‹è½½ `.edf` å’Œ `.lab`
            download_file(full_url, local_path)


# å¼€å§‹é€’å½’ä¸‹è½½ï¼ˆä»… `edf/` åŠå…¶å­ç›®å½•ï¼‰
recursive_download(base_url, save_dir)
print("ğŸ‰ å…¨éƒ¨ä¸‹è½½å®Œæˆï¼")
'''

import os
import time
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from urllib.parse import urljoin, urlparse

# è®¤è¯ä¿¡æ¯
username = "nedc-tuh-eeg"
password = "L5!Np#8Z$7xJmRpK"

# ç›®æ ‡èµ·å§‹ç›®å½•ï¼ˆåªä¸‹è½½ `edf/` åŠä»¥ä¸‹å†…å®¹ï¼‰
base_url = "https://isip.piconepress.com/projects/nedc/data/tuh_eeg/tuh_eeg_events/v2.0.1/edf/"
save_dir = "./downloads/tuh_eeg_events_v2.0.1"

# è®°å½•å·²è®¿é—®çš„ç›®å½•ï¼Œé¿å…æ— é™é€’å½’
visited_dirs = set()

# åˆ›å»ºä¼šè¯ï¼Œå¸¦ä¸Šèº«ä»½è®¤è¯
session = requests.Session()
session.auth = (username, password)

# **å…¨å±€è¯·æ±‚å‚æ•°**
MAX_RETRIES = 5  # å¤±è´¥æ—¶æœ€å¤§é‡è¯•æ¬¡æ•°
TIMEOUT = 15  # è¶…æ—¶æ—¶é—´
DELAY_BETWEEN_REQUESTS = 2  # è®¿é—®é—´éš”ï¼Œé˜²æ­¢è¯·æ±‚è¿‡å¿«è¢«å°é”


def get_file_links(url):
    """ è·å–ç½‘é¡µä¸­æ‰€æœ‰æ–‡ä»¶å’Œå­ç›®å½•é“¾æ¥ """
    retries = 0
    while retries < MAX_RETRIES:
        try:
            response = session.get(url, timeout=TIMEOUT)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            links = []
            for link in soup.find_all("a"):
                href = link.get("href")
                full_url = urljoin(url, href)
                # **è¿‡æ»¤æ‰æ— æ•ˆé“¾æ¥**
                if href and not href.startswith("?") and not href.startswith("Parent Directory") and not href.startswith("../"):
                    if full_url.startswith(base_url):  # ç¡®ä¿ä¸è·³å‡º `edf/` ç›®å½•
                        links.append(href)

            time.sleep(DELAY_BETWEEN_REQUESTS)  # **é¿å…è¯·æ±‚è¿‡å¿«**
            return links

        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥ ({retries + 1}/{MAX_RETRIES})ï¼Œé”™è¯¯: {e}")
            time.sleep(5)  # å¤±è´¥åç­‰å¾… 5 ç§’å†é‡è¯•
            retries += 1

    print(f"âŒ æ— æ³•è·å– {url}ï¼Œè·³è¿‡...")
    return []  # æœ€ç»ˆè¯·æ±‚å¤±è´¥åˆ™è¿”å›ç©ºåˆ—è¡¨


def sanitize_filename(url):
    """ å¤„ç†éæ³•æ–‡ä»¶å """
    parsed_url = urlparse(url)
    filename = os.path.basename(parsed_url.path)
    return filename.replace("?", "_").replace(";", "_").replace("=", "_")


def download_file(file_url, local_path):
    """ ä¸‹è½½å•ä¸ªæ–‡ä»¶ï¼Œæ”¯æŒè‡ªåŠ¨é‡è¯• """
    if os.path.exists(local_path):
        print(f"âœ… å·²å­˜åœ¨ï¼Œè·³è¿‡: {local_path}")
        return

    retries = 0
    while retries < MAX_RETRIES:
        try:
            print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½: {file_url}")
            response = session.get(file_url, stream=True, timeout=TIMEOUT)
            response.raise_for_status()

            with open(local_path, "wb") as f:
                for chunk in tqdm(response.iter_content(chunk_size=8192), desc=os.path.basename(local_path)):
                    f.write(chunk)

            time.sleep(DELAY_BETWEEN_REQUESTS)  # **é¿å…è¯·æ±‚è¿‡å¿«**
            return

        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ ä¸‹è½½å¤±è´¥ ({retries + 1}/{MAX_RETRIES})ï¼Œé”™è¯¯: {e}")
            time.sleep(5)  # å¤±è´¥åç­‰å¾… 5 ç§’å†é‡è¯•
            retries += 1

    print(f"âŒ æ— æ³•ä¸‹è½½ {file_url}ï¼Œè·³è¿‡...")


def recursive_download(url, local_dir):
    """ é€’å½’ä¸‹è½½æ•´ä¸ª `edf/` åŠä»¥ä¸‹çš„ `train/` `eval/` ç›®å½•å’Œ `.edf` `.lab` `.rec` `.htk` æ–‡ä»¶ """
    if url in visited_dirs:
        print(f"âš ï¸ å·²è®¿é—®ï¼Œè·³è¿‡: {url}")
        return  # é¿å…æ­»å¾ªç¯

    visited_dirs.add(url)  # è®°å½•å½“å‰è®¿é—®çš„ç›®å½•
    os.makedirs(local_dir, exist_ok=True)  # **ä¿æŒåŸå§‹ç›®å½•ç»“æ„**

    links = get_file_links(url)

    for link in links:
        full_url = urljoin(url, link)
        local_path = os.path.join(local_dir, link)  # **ä¿æŒåŸå§‹ç›®å½•ç»“æ„**

        if link.endswith("/") and full_url.startswith(base_url):  # ç›®å½•ï¼Œé€’å½’ä¸‹è½½
            print(f"ğŸ“‚ è¿›å…¥å­ç›®å½•: {full_url}")
            recursive_download(full_url, local_path)
        elif (link.endswith(".edf") or link.endswith(".lab") or link.endswith(".rec") or link.endswith(".htk")) and full_url.startswith(base_url):  # ä¸‹è½½ `.edf` `.lab` `.rec` `.htk`
            download_file(full_url, local_path)


# å¼€å§‹é€’å½’ä¸‹è½½ï¼ˆä»… `edf/` åŠå…¶å­ç›®å½•ï¼‰
recursive_download(base_url, save_dir)
print("ğŸ‰ å…¨éƒ¨ä¸‹è½½å®Œæˆï¼")
